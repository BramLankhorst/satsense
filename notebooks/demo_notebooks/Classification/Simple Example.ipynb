{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 200, 200\n",
    "\n",
    "windows = (\n",
    "    (50, 50),\n",
    "#     (100, 100),\n",
    "#     (200, 200),\n",
    ")\n",
    "\n",
    "train_files = (\n",
    "    '/home/bandela/DynaSlum/Work/section_1_multiband.tif',\n",
    "    '/home/bandela/DynaSlum/Work/section_2_multiband.tif',\n",
    ")\n",
    "\n",
    "test_files = (\n",
    "    '/home/bandela/DynaSlum/Work/section_3_multiband.tif',\n",
    ")\n",
    "\n",
    "ground_truth_file = '/home/bandela/DynaSlum/Work/slum_approved.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the set of features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satsense.features import (NirNDVI, HistogramOfGradients, Pantex, Sift,\n",
    "                               Lacunarity, Texton)\n",
    "from satsense import Image\n",
    "\n",
    "train_images = [Image(file, 'worldview3') for file in train_files]\n",
    "\n",
    "ndvi = NirNDVI(windows)\n",
    "hog = HistogramOfGradients(windows)\n",
    "pantex = Pantex(windows)\n",
    "lacunarity = Lacunarity(windows)\n",
    "sift = Sift.from_images(windows, train_images)\n",
    "texton = Texton.from_images(windows, train_images)\n",
    "\n",
    "features = [\n",
    "    ndvi,\n",
    "    hog,\n",
    "    pantex,\n",
    "    lacunarity,\n",
    "    sift,\n",
    "    texton,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from satsense import extract_features_parallel\n",
    "from satsense.generators import FullGenerator\n",
    "\n",
    "def compute_features(filenames):\n",
    "    paths = []\n",
    "    for filename in filenames:\n",
    "        image = Image(filename, 'worldview3')\n",
    "        path = Path(filename).stem + os.sep\n",
    "        paths.append(path)        \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            generator = FullGenerator(image, cell_size)\n",
    "            for feature_vector in extract_features_parallel(features, generator):\n",
    "                feature_vector.save(path)\n",
    "    return paths\n",
    "        \n",
    "train_data_paths = compute_features(train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from satsense import Image\n",
    "from satsense.util.mask import get_ndxi_mask, load_mask_from_shapefile, resample\n",
    "from satsense.features import NirNDVI, WVSI\n",
    "from satsense.generators import FullGenerator\n",
    "\n",
    "labels = {\n",
    "    'other': 0,\n",
    "    'deprived_neighbourhood': 1,\n",
    "    'vegetation': 2,\n",
    "}\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for path, image in zip(train_data_paths, [Image(f, 'worldview3') for f in train_files]):\n",
    "    # Load feature vector\n",
    "    feature_vector = []\n",
    "    for feature in features:\n",
    "        fv = FeatureVector.from_file(feature, path)\n",
    "        feature_vector.append(fv.vector)\n",
    "    feature_vector = np.ma.vstack(feature_vector)\n",
    "\n",
    "    label_vector = np.zeros(feature_vector.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Create vegetation labels\n",
    "    generator = FullGenerator(image, cell_size)\n",
    "    vegetation_mask = get_ndxi_mask(generator, NirNDVI)\n",
    "    label_vector[vegetation_mask] = labels['vegetation']\n",
    "    \n",
    "    # Create deprived neighbourhood labels \n",
    "    ground_truth = load_mask_from_shapefile(ground_truth_file, image.shape, image.transform)\n",
    "    ground_truth = resample(CellGenerator(Image(ground_truth), cell_size))\n",
    "    label_vector[ground_truth] = labels['deprived_neighbourhood']\n",
    "    \n",
    "    # Create x_train and y_train\n",
    "    feature_vector.shape = (-1, feature_vector.shape[2])\n",
    "    label_vector.shape = (-1, )\n",
    "\n",
    "    x_train.append(feature_vector)\n",
    "    y_train.append(label_vector)\n",
    "    \n",
    "x_train = np.concatenate(x_train)\n",
    "y_train = np.concatenate(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier(verbose=True)\n",
    "    \n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data and assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "test_data_paths = compute_features(test_files)\n",
    "\n",
    "for path, image in zip(test_data_paths, get_image_iterator(test_files)):\n",
    "    print('Performance on', image.name)\n",
    "    # Create x_test\n",
    "    x_test = load_features(features, path)\n",
    "    shape = x_test.shape\n",
    "    x_test.shape = (-1, shape[2])\n",
    "    \n",
    "    # Predict the labels\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    # Create y_test\n",
    "    y_test = np.zeros(shape[:2], dtype=np.uint8)\n",
    "    # Create deprived neighbourhood labels \n",
    "    ground_truth = load_mask_from_shapefile(ground_truth_file, image.shape, image.transform)\n",
    "    ground_truth = resample(CellGenerator(Image(ground_truth), cell_size))\n",
    "    y_test[ground_truth] = labels['deprived_neighbourhood']\n",
    "    # Create vegetation labels\n",
    "    generator = CellGenerator(image, cell_size)\n",
    "    vegetation_mask = get_ndxi_mask(generator, NirNDVI)\n",
    "    y_test[vegetation_mask] = labels['vegetation']\n",
    "    y_test.shape = (-1, )\n",
    "    \n",
    "    # Assess performance\n",
    "\n",
    "    # Label the vegetation as buildings to create more accurate representation of the performance\n",
    "    # y_pred[y_pred == labels['vegetation']] = labels['other']\n",
    "    # y_test[y_test == labels['vegetation']] = labels['other']\n",
    "\n",
    "    print(matthews_corrcoef(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, labels=list(labels.values()), target_names=list(labels.keys())))\n",
    "    print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
