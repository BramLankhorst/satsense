{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satsense import WORLDVIEW3, SatelliteImage\n",
    "\n",
    "cell_size = 20, 20\n",
    "\n",
    "windows = (\n",
    "    (50, 50),\n",
    "    (100, 100),\n",
    "    (200, 200),\n",
    ")\n",
    "\n",
    "train_files = (\n",
    "    '/home/bandela/DynaSlum/Work/section_1_multiband.tif',\n",
    "    '/home/bandela/DynaSlum/Work/section_2_multiband.tif',\n",
    ")\n",
    "\n",
    "test_files = (\n",
    "    '/home/bandela/DynaSlum/Work/section_3_multiband.tif',\n",
    ")\n",
    "\n",
    "ground_truth_file = '/home/bandela/DynaSlum/Work/slum_approved.shp'\n",
    "\n",
    "def get_image_iterator(files):\n",
    "    return (SatelliteImage.load_from_file(f, WORLDVIEW3) for f in files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the set of features for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from satsense.features import (FeatureSet, NirNDVI, HistogramOfGradients, Pantex, Sift,\n",
    "                               Lacunarity, Texton, sift_cluster, texton_cluster)\n",
    "\n",
    "features = FeatureSet()\n",
    "\n",
    "ndvi = NirNDVI(windows)\n",
    "features.add(ndvi)\n",
    "\n",
    "hog = HistogramOfGradients(windows)\n",
    "features.add(hog)\n",
    "\n",
    "pantex = Pantex(windows)\n",
    "features.add(pantex)\n",
    "\n",
    "lacunarity = Lacunarity(windows)\n",
    "features.add(lacunarity)\n",
    "\n",
    "sift = Sift(windows=windows, kmeans=sift_cluster(get_image_iterator(train_files)))\n",
    "features.add(sift)\n",
    "\n",
    "texton = Texton(windows=windows, kmeans=texton_cluster(get_image_iterator(train_files)))\n",
    "features.add(texton)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from satsense import extract_features_parallel, save_features\n",
    "from satsense.generators import CellGenerator\n",
    "\n",
    "def compute_features(filenames):\n",
    "    paths = []\n",
    "    for image in get_image_iterator(filenames):\n",
    "        path = os.path.splitext(os.path.basename(image.name))[0] + os.sep\n",
    "        paths.append(path)        \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            generator = CellGenerator(image, cell_size)\n",
    "            results = extract_features_parallel(features, generator)\n",
    "            save_features(features, results, filename_prefix=path)\n",
    "    return paths\n",
    "        \n",
    "train_data_paths = compute_features(train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from satsense import load_features\n",
    "from satsense.image import Image\n",
    "from satsense.util.mask import get_ndxi_mask, load_mask_from_shapefile, resample\n",
    "from satsense.features import NirNDVI, WVSI\n",
    "from satsense.generators import CellGenerator\n",
    "\n",
    "labels = {\n",
    "    'other': 0,\n",
    "    'deprived_neighbourhood': 1,\n",
    "    'vegetation': 2,\n",
    "}\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for path, image in zip(train_data_paths, get_image_iterator(train_files)):\n",
    "    # Load feature vector\n",
    "    feature_vector = load_features(features, path)\n",
    "    label_vector = np.zeros(feature_vector.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Create vegetation labels\n",
    "    generator = CellGenerator(image, cell_size)\n",
    "    vegetation_mask = get_ndxi_mask(generator, NirNDVI)\n",
    "    label_vector[vegetation_mask] = labels['vegetation']\n",
    "    \n",
    "    # Create deprived neighbourhood labels \n",
    "    ground_truth = load_mask_from_shapefile(ground_truth_file, image.shape, image.transform)\n",
    "    ground_truth = resample(CellGenerator(Image(ground_truth), cell_size))\n",
    "    label_vector[ground_truth] = labels['deprived_neighbourhood']\n",
    "    \n",
    "    # Create x_train and y_train\n",
    "    feature_vector.shape = (-1, feature_vector.shape[2])\n",
    "    label_vector.shape = (-1, )\n",
    "\n",
    "    x_train.append(feature_vector)\n",
    "    y_train.append(label_vector)\n",
    "    \n",
    "x_train = np.concatenate(x_train)\n",
    "y_train = np.concatenate(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier(verbose=True)\n",
    "    \n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data and assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "test_data_paths = compute_features(test_files)\n",
    "\n",
    "for path, image in zip(test_data_paths, get_image_iterator(test_files)):\n",
    "    print('Performance on', image.name)\n",
    "    # Create x_test\n",
    "    x_test = load_features(features, path)\n",
    "    shape = x_test.shape\n",
    "    x_test.shape = (-1, shape[2])\n",
    "    \n",
    "    # Predict the labels\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    # Create y_test\n",
    "    y_test = np.zeros(shape[:2], dtype=np.uint8)\n",
    "    # Create deprived neighbourhood labels \n",
    "    ground_truth = load_mask_from_shapefile(ground_truth_file, image.shape, image.transform)\n",
    "    ground_truth = resample(CellGenerator(Image(ground_truth), cell_size))\n",
    "    y_test[ground_truth] = labels['deprived_neighbourhood']\n",
    "    # Create vegetation labels\n",
    "    generator = CellGenerator(image, cell_size)\n",
    "    vegetation_mask = get_ndxi_mask(generator, NirNDVI)\n",
    "    y_test[vegetation_mask] = labels['vegetation']\n",
    "    y_test.shape = (-1, )\n",
    "    \n",
    "    # Assess performance\n",
    "\n",
    "    # Label the vegetation as buildings to create more accurate representation of the performance\n",
    "    # y_pred[y_pred == labels['vegetation']] = labels['other']\n",
    "    # y_test[y_test == labels['vegetation']] = labels['other']\n",
    "\n",
    "    print(matthews_corrcoef(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, labels=list(labels.values()), target_names=list(labels.keys())))\n",
    "    print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:satsense]",
   "language": "python",
   "name": "conda-env-satsense-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
